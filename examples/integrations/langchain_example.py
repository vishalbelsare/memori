#!/usr/bin/env python3
"""
LangChain Integration Example for Memoriai v1.0
Demonstrates automatic conversation recording with LangChain and Memoriai
"""

import os
from memoriai import Memori, create_memory_search_tool
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


def main():
    print("ğŸ¦œ Memoriai v1.0 + LangChain Integration Example")
    print("=" * 50)

    # Check if LangChain is available
    try:
        from langchain_openai import ChatOpenAI
        from langchain.schema import HumanMessage, SystemMessage

        print("âœ… LangChain is available")
    except ImportError:
        print(
            "âŒ LangChain not installed. Install with: pip install langchain langchain-openai"
        )
        print("   This example requires LangChain for integration testing")
        return

    # Initialize Memoriai for LangChain integration
    langchain_memory = Memori(
        database_connect="sqlite:///langchain_memory.db",
        template="basic",
        mem_prompt="Remember LangChain conversations, AI chains, and agent interactions",
        conscious_ingest=True,
        namespace="langchain_experiments",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )

    print("âœ… Memoriai initialized for LangChain integration")

    # Enable auto-recording (may capture some LangChain calls)
    langchain_memory.enable()
    print("âœ… Auto-recording enabled!")
    print(f"ğŸ“Š Session ID: {langchain_memory.session_id}")

    # Example 1: Basic LangChain conversation
    print("\nğŸ¦œ Example 1: Basic LangChain conversation...")

    try:
        # Initialize LangChain ChatOpenAI
        chat = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=os.getenv("OPENAI_API_KEY"),
        )

        # Create messages
        messages = [
            SystemMessage(
                content="You are a helpful AI assistant specializing in LangChain and AI development."
            ),
            HumanMessage(
                content="What are the main components of LangChain and how do they work together?"
            ),
        ]

        # Get response (may be auto-recorded depending on LangChain integration)
        response = chat.invoke(messages)
        ai_response = response.content

        print("âœ… LangChain conversation completed!")
        print(f"ğŸ“„ Response preview: {ai_response[:100]}...")

        # Manual recording to ensure it's captured
        chat_id = langchain_memory.record_conversation(
            user_input="What are the main components of LangChain and how do they work together?",
            ai_output=ai_response,
            model="langchain-gpt-3.5-turbo",
        )
        print(f"âœ… Manually recorded conversation: {chat_id[:8]}...")

    except Exception as e:
        print(f"âŒ LangChain conversation failed: {e}")
        return

    # Example 2: Chain-based conversation
    print("\nâ›“ï¸  Example 2: LangChain Chain conversation...")

    try:
        from langchain.prompts import PromptTemplate
        from langchain.chains import LLMChain

        # Create a prompt template
        prompt = PromptTemplate(
            input_variables=["topic"],
            template="Explain {topic} in the context of building AI applications with LangChain. Focus on practical implementation.",
        )

        # Create an LLM chain
        chain = LLMChain(llm=chat, prompt=prompt)

        # Run the chain
        topics = ["vector databases", "prompt engineering", "agent architectures"]

        for topic in topics:
            try:
                result = chain.run(topic=topic)
                print(f"   ğŸ”— Chain result for '{topic}': {result[:50]}...")

                # Record the chain interaction
                chain_chat_id = langchain_memory.record_conversation(
                    user_input=f"Explain {topic} in the context of building AI applications with LangChain",
                    ai_output=result,
                    model="langchain-chain-gpt-3.5-turbo",
                )
                print(f"   âœ… Recorded chain conversation: {chain_chat_id[:8]}...")

            except Exception as e:
                print(f"   âŒ Chain failed for {topic}: {e}")
                continue

    except ImportError:
        print("âŒ LangChain chains not available, skipping chain example")
    except Exception as e:
        print(f"âŒ Chain example failed: {e}")

    # Example 3: Memory-augmented responses
    print("\nğŸ§  Example 3: Memory-augmented LangChain responses...")

    # Create memory search tool
    memory_search_tool = create_memory_search_tool(langchain_memory)

    # Search for relevant memories
    search_result = memory_search_tool("LangChain components", max_results=2)
    print("ğŸ“‹ Memory search for LangChain components:")

    import json

    try:
        search_data = json.loads(search_result)
        if search_data.get("found", 0) > 0:
            print(f"   ğŸ“Š Found {search_data['found']} relevant memories")
            for i, memory in enumerate(search_data["memories"], 1):
                summary = memory.get("summary", "No summary")
                print(f"   {i}. {summary[:70]}...")
        else:
            print("   ğŸ“­ No relevant memories found yet")
    except json.JSONDecodeError:
        print(f"   ğŸ“„ Raw result: {search_result[:100]}...")

    # Example 4: Agent-style interaction with memory context
    print("\nğŸ¤– Example 4: Agent-style interaction with memory...")

    try:
        # Get memory context for the next conversation
        memory_context = langchain_memory.retrieve_context("vector databases", limit=2)

        context_text = ""
        if memory_context:
            context_text = "\n".join(
                [f"- {item.get('summary', 'No summary')}" for item in memory_context]
            )

        # Create context-aware prompt
        context_prompt = f"""
You are an AI assistant with access to previous conversation context.

Previous relevant context:
{context_text if context_text else "No previous context available"}

Based on this context and your knowledge, please respond to the user's question.
"""

        messages_with_context = [
            SystemMessage(content=context_prompt),
            HumanMessage(
                content="How should I choose between different vector database solutions for my LangChain application?"
            ),
        ]

        # Get context-aware response
        context_response = chat.invoke(messages_with_context)
        context_ai_response = context_response.content

        print("âœ… Context-aware response generated!")
        print(f"ğŸ“„ Context-aware response preview: {context_ai_response[:100]}...")

        # Record the context-aware conversation
        context_chat_id = langchain_memory.record_conversation(
            user_input="How should I choose between different vector database solutions for my LangChain application?",
            ai_output=context_ai_response,
            model="langchain-context-aware-gpt-3.5-turbo",
        )
        print(f"âœ… Context-aware conversation recorded: {context_chat_id[:8]}...")

    except Exception as e:
        print(f"âŒ Context-aware conversation failed: {e}")

    # Wait for memory processing
    import time

    time.sleep(2)

    # Example 5: LangChain integration statistics
    print("\nğŸ“ˆ Example 5: LangChain integration statistics...")

    stats = langchain_memory.get_memory_stats()
    integration_stats = langchain_memory.get_integration_stats()

    print("Memory Statistics:")
    print(f"  ğŸ’¬ Total Conversations: {stats.get('chat_history_count', 0)}")
    print(
        f"  ğŸ§  Total Memories: {stats.get('short_term_count', 0) + stats.get('long_term_count', 0)}"
    )
    print(f"  ğŸ·ï¸  Total Entities: {stats.get('total_entities', 0)}")

    categories = stats.get("memories_by_category", {})
    if categories:
        print(f"  ğŸ“Š Categories: {dict(categories)}")

    print("Integration Statistics:")
    print(f"  ğŸ”— Active Integrations: {len(integration_stats)}")
    for stat in integration_stats:
        integration_name = stat.get("integration", "unknown")
        active_instances = stat.get("active_instances", 0)
        print(f"  ğŸ“¡ {integration_name}: {active_instances} instances")

    # Example 6: Export LangChain memories
    print("\nğŸ’¾ Example 6: Export LangChain memories...")

    # Get conversation history
    recent_conversations = langchain_memory.get_conversation_history(limit=5)
    print(f"ğŸ“š Recent conversations: {len(recent_conversations)}")

    langchain_export = {
        "namespace": langchain_memory.namespace,
        "session_id": langchain_memory.session_id,
        "conversations": len(recent_conversations),
        "memory_stats": stats,
        "export_timestamp": time.time(),
    }

    print("ğŸ“„ Export structure created for LangChain memories")

    # Example 7: Future integration possibilities
    print("\nğŸš€ Example 7: Future integration possibilities...")

    print("ğŸ’¡ Potential LangChain + Memoriai integrations:")
    print("   ğŸ”— Custom memory retriever for LangChain chains")
    print("   ğŸ§  Context-aware agent memory")
    print("   ğŸ“Š Conversation analytics and insights")
    print("   ğŸ” Semantic search across chain interactions")
    print("   âš¡ Real-time memory updates during chain execution")

    # Clean up
    langchain_memory.disable()
    print("\nğŸ”’ LangChain integration disabled")

    print("\nğŸ’¡ Integration Summary:")
    print("   âœ… Manual conversation recording")
    print("   âœ… Chain interaction capture")
    print("   âœ… Memory-augmented responses")
    print("   âœ… Context-aware conversations")
    print("   âœ… Integration statistics")
    print("   âœ… Memory export capabilities")

    print("\nğŸ‰ LangChain integration example completed!")
    print("ğŸ’¾ Check 'langchain_memory.db' for recorded interactions")


if __name__ == "__main__":
    main()
